{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Preparación previa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura del dataset original de Properati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://media.githubusercontent.com/media/Agustin-Bulzomi/Projects/main/Programming/Digital%20House%20(Python)/Support%20Files/Project%201/Properati.csv\", index_col=0)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de nulos según columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulos = data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulos_porcentaje = nulos / data.shape[0] * 100\n",
    "nulos_porcentaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base a los resultados se llegó a las siguientes conclusiones:\n",
    "\n",
    "1) Se tomará la superficie cubierta en vez de la total. En el anterior desafío se creó una función para tomar una mezcla de ambas ignorando las inconsistencias pero daba más nulos aún.\n",
    "\n",
    "2) Se tomará el precio aprox en dólares en vez del precio per m2 en dólares. Al mismo se dividirá por la superficie cubierta para tener el valor por m2\n",
    "\n",
    "3) \"rooms\" tiene pocos valores, se tendrá que imputar los datos faltantes\n",
    "\n",
    "4) \"description\" y \"title\" servirán para obtener información extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separación de columna con muchas ubicaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La columna \"place_with_parent_names\" tiene información separada con '|'. Se separa para obtener info adicional\n",
    "separar_zona = data[\"place_with_parent_names\"].str.split('|', expand = True)\n",
    "separar_zona.columns = ['??', 'Pais', 'Zona', 'Partido', 'Barrios', 'Country', 'Otra']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregado de la nueva información en nuevas columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concat = pd.concat([data, separar_zona], axis=1)\n",
    "data_concat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de la columna de precios por metros cuadrados usando la superficie cubierta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concat['precio_usd_por_m2'] = data_concat.price_aprox_usd/data_concat.surface_covered_in_m2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Imputación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a la insuficiente cantidad de datos de ambientes, vamos a intentar obtener más"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Imputación en base a título y descripción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patron_amb = \"(?P<ambiente>\\d\\s)((A|a)(M|m)(B|b))\"\n",
    "regex_amb = re.compile(patron_amb)\n",
    "\n",
    "data_amb_serie = data_concat[\"description\"]\n",
    "data_amb_match = data_amb_serie.apply(lambda x: x if x is np.NaN else regex_amb.search(x))\n",
    "\n",
    "mask_amb_notnull = data_amb_match.notnull()\n",
    "\n",
    "data_ambientes = data_amb_match[mask_amb_notnull].apply(lambda x: x.group(\"ambiente\"))\n",
    "\n",
    "data_concat.loc[mask_amb_notnull, 'ambientes_desc'] = \\\n",
    "    data_amb_match[mask_amb_notnull].apply(lambda x: x.group('ambiente'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concat.loc[mask_amb_notnull, [\"description\", \"ambientes_desc\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Título"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patron_amb2 = \"(?P<ambiente_title>\\d\\s)((A|a)(M|m)(B|b))\"\n",
    "regex_amb2 = re.compile(patron_amb2)\n",
    "\n",
    "data_amb_serie2 = data_concat[\"title\"]\n",
    "data_amb_match2 = data_amb_serie2.apply(lambda x: x if x is np.NaN else regex_amb2.search(x))\n",
    "\n",
    "mask_amb_notnull2 = data_amb_match2.notnull()\n",
    "\n",
    "data_ambientes2 = data_amb_match2[mask_amb_notnull2].apply(lambda x: x.group(\"ambiente_title\"))\n",
    "\n",
    "data_concat.loc[mask_amb_notnull2, 'ambientes_t'] = \\\n",
    "    data_amb_match2[mask_amb_notnull2].apply(lambda x: x.group('ambiente_title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concat.loc[mask_amb_notnull2, [\"ambientes_desc\", \"ambientes_t\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unificación de la nueva información"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una función para resumir ambas columnas en una nueva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza_amb(ambientes_desc, ambientes_t):\n",
    "    if pd.isnull(ambientes_desc) and pd.isnull(ambientes_t):\n",
    "        ambientes = np.NaN\n",
    "    elif pd.isnull(ambientes_desc):\n",
    "        ambientes = ambientes_t\n",
    "    else:\n",
    "        ambientes = ambientes_desc\n",
    "    return ambientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se aplica la función\n",
    "data_concat[\"ambientes\"] = data_concat.apply(lambda data_concat: limpieza_amb(data_concat['ambientes_desc'],data_concat['ambientes_t']),axis=1)\n",
    "data_concat.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concat.ambientes.notnull().sum() / data_concat.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se  crea una función para resumir la información entre la nueva columna y rooms.\n",
    "\n",
    "En la enorme mayoría de los casos en donde se tenía el dato de rooms original, la cantidad de ambientes obtenida por imputación concordaba con el valor de rooms original. Esto indica que ambos términos son intercambiables al menos en este dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza_amb2(rooms, ambientes):\n",
    "    if pd.isnull(rooms) and pd.isnull(ambientes):\n",
    "        ambientes_train = 0\n",
    "    elif pd.isnull(rooms):\n",
    "        ambientes_train = ambientes\n",
    "    else:\n",
    "        ambientes_train = int(rooms)\n",
    "    return int(ambientes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se aplica la función. Se llama a la nueva variable \"train\" pues es la que será usada para entrenar al modelo\n",
    "data_concat[\"ambientes_train\"] = data_concat.apply(lambda x: limpieza_amb2(x['rooms'],x['ambientes']),axis=1)\n",
    "data_concat.ambientes_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Imputación en base a la superficie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomando la mediana de las superficies agrupadas según ambientes definir un punto medio entre cada mediana.\n",
    "\n",
    "La misma nos permitiría definir un divisor que delimite cuándo una superficie es más probable que pertenezca a una cantidad de ambientes. Al ser una imputación no tan certera, se dejará afuera de la serie \"train\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se calcula cuánta información nueva podría obtenerse\n",
    "superficie_not_null = data_concat['surface_covered_in_m2'].notnull()\n",
    "ambientes_zero = data_concat['ambientes_train'] == 0\n",
    "filtro = superficie_not_null & ambientes_zero\n",
    "print(filtro.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisores de ambientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amb_1 = data_concat['ambientes_train'] == 1\n",
    "amb_2 = data_concat['ambientes_train'] == 2\n",
    "amb_3 = data_concat['ambientes_train'] == 3\n",
    "amb_4 = data_concat['ambientes_train'] == 4\n",
    "amb_5 = data_concat['ambientes_train'] == 5\n",
    "amb_6 = data_concat['ambientes_train'] == 6\n",
    "amb_7 = data_concat['ambientes_train'] == 7\n",
    "\n",
    "divisor1 = (data_concat[amb_1].surface_covered_in_m2.median() + data_concat[amb_2].surface_covered_in_m2.median())/2\n",
    "divisor2 = (data_concat[amb_2].surface_covered_in_m2.median() + data_concat[amb_3].surface_covered_in_m2.median())/2\n",
    "divisor3 = (data_concat[amb_3].surface_covered_in_m2.median() + data_concat[amb_4].surface_covered_in_m2.median())/2\n",
    "divisor4 = (data_concat[amb_4].surface_covered_in_m2.median() + data_concat[amb_5].surface_covered_in_m2.median())/2\n",
    "divisor5 = (data_concat[amb_5].surface_covered_in_m2.median() + data_concat[amb_6].surface_covered_in_m2.median())/2\n",
    "divisor6 = (data_concat[amb_6].surface_covered_in_m2.median() + data_concat[amb_7].surface_covered_in_m2.median())/2\n",
    "divisor7 = (data_concat[amb_7].surface_covered_in_m2.median() + data_concat[amb_7].surface_covered_in_m2.max())/2\n",
    "\n",
    "# Como no hay de 8 ambientes, se utiliza el valor máximo de 7 ambientes como tope para calcular el divisor 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea una función para asignar ambientes según los divisores\n",
    "\n",
    "def asignar_ambientes_segun_superficie(surface_covered_in_m2):\n",
    "    #if superficie.isnull():\n",
    "    #    return 0\n",
    "    #elif 0 < superficie <= divisor1:\n",
    "    if 0 < surface_covered_in_m2 <= divisor1:\n",
    "        return 1\n",
    "    elif divisor1 < surface_covered_in_m2 <= divisor2:\n",
    "        return 2\n",
    "    elif divisor2 < surface_covered_in_m2 <= divisor3:\n",
    "        return 3\n",
    "    elif divisor3 < surface_covered_in_m2 <= divisor4:\n",
    "        return 4\n",
    "    elif divisor4 < surface_covered_in_m2 <= divisor5:\n",
    "        return 5\n",
    "    elif divisor5 < surface_covered_in_m2 <= divisor6:\n",
    "        return 6\n",
    "    elif divisor6 < surface_covered_in_m2 <= divisor7:\n",
    "        return 7\n",
    "    else:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se aplica la función para crear una columna de ambientes imputados con valores en las filas que no tienen valores de ambientes_train\n",
    "\n",
    "data_concat[\"ambientes_imputados\"] = data_concat.apply(lambda x: asignar_ambientes_segun_superficie(x['surface_covered_in_m2']) if int(x['ambientes_train']) == 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concat.ambientes_imputados.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se suman ambas columnas al ser excluyentes: ambientes_final no tiene 0, cada fila tiene un valor original o imputado\n",
    "\n",
    "data_concat[\"ambientes_final\"] = data_concat[\"ambientes_train\"] + data_concat[\"ambientes_imputados\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Búsqueda de amenities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se analiza la descripción de cada fila para encontrar palabras clave que indiquen amenities con valor agregado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patron_balcon = \"(?P<balcon>(B|b)(A|a)(L|l)(C|c)(O|n)(N|n))\"\n",
    "regex_balcon = re.compile(patron_balcon)\n",
    "data_balcon = data_concat[\"description\"]\n",
    "data_match_balcon = data_balcon.apply(lambda x: x if x is np.NaN else regex_balcon.search(x))\n",
    "mask_notnull_balcon = data_match_balcon.notnull()\n",
    "data_balcon = data_match_balcon[mask_notnull_balcon].apply(lambda x: x.group(\"balcon\"))\n",
    "data_concat.loc[mask_notnull_balcon, 'balcon'] = \\\n",
    "data_match_balcon[mask_notnull_balcon].apply(lambda x: x.group('balcon').lower())\n",
    "\n",
    "patron_parrilla = \"(?P<parrilla>(P|p)(A|a)(R|r)(R|r)(I|i)(L|l)(L|l)(A|a))\"\n",
    "regex_parrilla = re.compile(patron_parrilla)\n",
    "data_parrilla = data_concat[\"description\"]\n",
    "data_match_parrilla = data_parrilla.apply(lambda x: x if x is np.NaN else regex_parrilla.search(x))\n",
    "mask_notnull_parrilla = data_match_parrilla.notnull()\n",
    "data_parrilla = data_match_parrilla[mask_notnull_parrilla].apply(lambda x: x.group(\"parrilla\"))\n",
    "data_concat.loc[mask_notnull_parrilla, 'parrilla'] = \\\n",
    "data_match_parrilla[mask_notnull_parrilla].apply(lambda x: x.group('parrilla').lower())\n",
    "\n",
    "patron_pileta = \"(?P<pileta>(P|p)(I|i)(L|l)(E|e)(T|t)(A|a))\"\n",
    "regex_pileta = re.compile(patron_pileta)\n",
    "data_pileta = data_concat[\"description\"]\n",
    "data_match_pileta = data_pileta.apply(lambda x: x if x is np.NaN else regex_pileta.search(x))\n",
    "mask_notnull_pileta = data_match_pileta.notnull()\n",
    "data_pileta = data_match_pileta[mask_notnull_pileta].apply(lambda x: x.group(\"pileta\"))\n",
    "data_concat.loc[mask_notnull_pileta, 'pileta'] = \\\n",
    "data_match_pileta[mask_notnull_pileta].apply(lambda x: x.group('pileta').lower())\n",
    "\n",
    "patron_patio = \"(?P<patio>(P|p)(A|a)(T|t)(I|i)(O|o))\"\n",
    "regex_patio = re.compile(patron_patio)\n",
    "data_patio = data_concat[\"description\"]\n",
    "data_match_patio = data_patio.apply(lambda x: x if x is np.NaN else regex_patio.search(x))\n",
    "mask_notnull_patio = data_match_patio.notnull()\n",
    "data_patio = data_match_patio[mask_notnull_patio].apply(lambda x: x.group(\"patio\"))\n",
    "data_concat.loc[mask_notnull_patio, 'patio'] = \\\n",
    "data_match_patio[mask_notnull_patio].apply(lambda x: x.group('patio').lower())\n",
    "\n",
    "patron_quincho = \"(?P<quincho>(Q|q)(U|u)(I|i)(N|n)(C|c)(H|h)(O|o))\"\n",
    "regex_quincho = re.compile(patron_quincho)\n",
    "data_quincho = data_concat[\"description\"]\n",
    "data_match_quincho = data_quincho.apply(lambda x: x if x is np.NaN else regex_quincho.search(x))\n",
    "mask_notnull_quincho = data_match_quincho.notnull()\n",
    "data_quincho = data_match_quincho[mask_notnull_quincho].apply(lambda x: x.group(\"quincho\"))\n",
    "data_concat.loc[mask_notnull_quincho, 'quincho'] = \\\n",
    "data_match_quincho[mask_notnull_quincho].apply(lambda x: x.group('quincho').lower())\n",
    "\n",
    "patron_gimnasio = \"(?P<gimnasio>(G|g)(I|i)(M|m)(N|n)(A|a)(C|c|S|s)(I|i)(O|o))\"\n",
    "regex_gimnasio = re.compile(patron_gimnasio)\n",
    "data_gimnasio = data_concat[\"description\"]\n",
    "data_match_gimnasio = data_gimnasio.apply(lambda x: x if x is np.NaN else regex_gimnasio.search(x))\n",
    "mask_notnull_gimnasio = data_match_gimnasio.notnull()\n",
    "data_gimnasio = data_match_gimnasio[mask_notnull_gimnasio].apply(lambda x: x.group(\"gimnasio\"))\n",
    "data_concat.loc[mask_notnull_gimnasio, 'gimnasio'] = \\\n",
    "data_match_gimnasio[mask_notnull_gimnasio].apply(lambda x: x.group('gimnasio').lower().replace(\"gimnacio\", \"gimnasio\"))\n",
    "\n",
    "patron_sum = \"(?P<sum>(S|s)(U|u)(M|m))\"\n",
    "regex_sum = re.compile(patron_sum)\n",
    "data_sum = data_concat[\"description\"]\n",
    "data_match_sum = data_sum.apply(lambda x: x if x is np.NaN else regex_sum.search(x))\n",
    "mask_notnull_sum = data_match_sum.notnull()\n",
    "data_sum = data_match_sum[mask_notnull_sum].apply(lambda x: x.group(\"sum\"))\n",
    "data_concat.loc[mask_notnull_sum, 'sala_usos_multiples'] = \\\n",
    "data_match_sum[mask_notnull_sum].apply(lambda x: x.group('sum').lower())\n",
    "\n",
    "patron_cochera = \"(?P<cochera>(C|c)(O|o)(C|c)(H|h)(E|e)(R|r)(A|a)|(E|e)(S|s)(T|t)(A|a)(C|c)(I|i)(O|o)(N|n)(A|a)(M|m)(I|i)(E|e)(N|n)(T|t)(O|o))\"\n",
    "regex_cochera = re.compile(patron_cochera)\n",
    "data_cochera = data_concat[\"description\"]\n",
    "data_match_cochera = data_cochera.apply(lambda x: x if x is np.NaN else regex_cochera.search(x))\n",
    "mask_notnull_cochera = data_match_cochera.notnull()\n",
    "data_cochera = data_match_cochera[mask_notnull_cochera].apply(lambda x: x.group(\"cochera\"))\n",
    "data_concat.loc[mask_notnull_cochera, 'cochera'] = \\\n",
    "data_match_cochera[mask_notnull_cochera].apply(lambda x: x.group('cochera').lower().replace(\"estacionamiento\", \"cochera\"))\n",
    "\n",
    "patron_seguridad = \"(?P<seguridad>(S|s)(E|e)(G|g)(U|u)(R|r)(I|i)(D|d)(A|a)(D|d)|(P|p)(O|o)(R|r)(T|t)(E|e)(R|r)(O|o))\"\n",
    "regex_seguridad = re.compile(patron_seguridad)\n",
    "data_seguridad = data_concat[\"description\"]\n",
    "data_match_seguridad = data_seguridad.apply(lambda x: x if x is np.NaN else regex_seguridad.search(x))\n",
    "mask_notnull_seguridad = data_match_seguridad.notnull()\n",
    "data_seguridad = data_match_seguridad[mask_notnull_seguridad].apply(lambda x: x.group(\"seguridad\"))\n",
    "data_concat.loc[mask_notnull_seguridad, 'seguridad'] = \\\n",
    "data_match_seguridad[mask_notnull_seguridad].apply(lambda x: x.group('seguridad').lower().replace(\"portero\", \"seguridad\"))\n",
    "\n",
    "patron_jardin = \"(?P<jardin>(J|j)(A|a)(R|r)(D|d)(I|i)(N|n))\"\n",
    "regex_jardin = re.compile(patron_jardin)\n",
    "data_jardin = data_concat[\"description\"]\n",
    "data_match_jardin = data_jardin.apply(lambda x: x if x is np.NaN else regex_jardin.search(x))\n",
    "mask_notnull_jardin = data_match_jardin.notnull()\n",
    "data_jardin = data_match_jardin[mask_notnull_jardin].apply(lambda x: x.group(\"jardin\"))\n",
    "data_concat.loc[mask_notnull_jardin, 'jardin'] = \\\n",
    "data_match_jardin[mask_notnull_jardin].apply(lambda x: x.group('jardin').lower())\n",
    "\n",
    "patron_frente = \"(?P<frente>(F|f)(R|r)(E|e)(N|n)(T|t)(E|e))\"\n",
    "regex_frente = re.compile(patron_frente)\n",
    "data_frente = data_concat[\"description\"]\n",
    "data_match_frente = data_frente.apply(lambda x: x if x is np.NaN else regex_frente.search(x))\n",
    "mask_notnull_frente = data_match_frente.notnull()\n",
    "data_frente = data_match_frente[mask_notnull_frente].apply(lambda x: x.group(\"frente\"))\n",
    "data_concat.loc[mask_notnull_frente, 'frente'] = \\\n",
    "data_match_frente[mask_notnull_frente].apply(lambda x: x.group('frente').lower())\n",
    "\n",
    "data_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Eliminación de nulos, ceros, outliers e información innecesaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos innecesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No es de interés para el análisis actual la información inmobiliaria de tiendas\n",
    "\n",
    "mask_not_store = data_concat['property_type'] != 'store'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concat = data_concat[mask_not_store]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columnas innecesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sin_columnas = data_concat.drop(['operation', 'place_with_parent_names', 'place_name', 'country_name', 'state_name',\n",
    "                                     'geonames_id', 'lat-lon', 'floor', 'expenses', 'properati_url', 'image_thumbnail', '??', 'price_usd_per_m2',\n",
    "                                     'place_name', 'currency', 'price_aprox_local_currency', 'surface_total_in_m2', 'price_per_m2',\n",
    "                                     'price_aprox_usd', \"lat\", \"lon\",  \"Country\", \"Otra\", \"Barrios\", \"Pais\", \"Zona\", \"ambientes_desc\",\n",
    "                                      \"ambientes_t\", \"ambientes\", \"rooms\", \"title\", \"description\"], axis = 1)\n",
    "data_sin_columnas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nulos y Ceros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se procede a eliminar en cada renglón las filas con nulos o ceros\n",
    "\n",
    "data_partido_not_null = data_sin_columnas.dropna(subset = [\"Partido\"], how = \"any\")\n",
    "data_partido_not_empty = data_partido_not_null[data_partido_not_null.Partido != \"\"]\n",
    "data_ambientes_not_zero = data_partido_not_empty[(data_partido_not_empty.ambientes_train > 0) | (data_partido_not_empty.ambientes_imputados != 0)]\n",
    "data_surface_not_zero = data_ambientes_not_zero[data_ambientes_not_zero.surface_covered_in_m2 > 0]\n",
    "data_surface_not_null = data_surface_not_zero.dropna(subset = [\"surface_covered_in_m2\"], how = \"any\")\n",
    "data_price_not_zero = data_surface_not_null[data_surface_not_null.precio_usd_por_m2 > 0]\n",
    "data_price_not_null = data_price_not_zero.dropna(subset = [\"precio_usd_por_m2\"], how = \"any\")\n",
    "data_price_not_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Superficie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_surface = data_price_not_null.surface_covered_in_m2.quantile(0.25)\n",
    "q2_surface = data_price_not_null.surface_covered_in_m2.quantile(0.5)\n",
    "q3_surface = data_price_not_null.surface_covered_in_m2.quantile(0.75)\n",
    "\n",
    "higher_bound_surface = q3_surface + 1.5 * (q3_surface - q1_surface)\n",
    "lower_bound_surface = q1_surface - 1.5 * (q3_surface - q1_surface)\n",
    "\n",
    "print(\"El límite inferior es \", lower_bound_surface, \" y el superior es \", higher_bound_surface)\n",
    "\n",
    "# Considerando que el límite inferior da negativo, se usará un estadístico propio para el límite inferior\n",
    "\n",
    "lower_bound_surface_nuevo = q1_surface.mean() * 0.25\n",
    "print(\"El nuevo límite inferior es\", lower_bound_surface_nuevo)\n",
    "\n",
    "outlier_mask_up = data_price_not_null.surface_covered_in_m2 < higher_bound_surface\n",
    "outlier_mask_down = data_price_not_null.surface_covered_in_m2 > lower_bound_surface_nuevo\n",
    "outlier_mask = np.logical_and(outlier_mask_up, outlier_mask_down)\n",
    "data_sin_outliers_superficie = data_price_not_null[outlier_mask]\n",
    "data_sin_outliers_superficie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_price = data_sin_outliers_superficie.precio_usd_por_m2.quantile(0.25)\n",
    "q2_price = data_sin_outliers_superficie.precio_usd_por_m2.quantile(0.5)\n",
    "q3_price = data_sin_outliers_superficie.precio_usd_por_m2.quantile(0.75)\n",
    "\n",
    "higher_bound_price = q3_price + 1.5 * (q3_price - q1_price)\n",
    "lower_bound_price = q1_price - 1.5 * (q3_price - q1_price)\n",
    "\n",
    "print(\"El límite inferior es \", lower_bound_price, \" y el superior es \", higher_bound_price)\n",
    "\n",
    "# Considerando que el número da negativo, se usará un estadístico propio para el límite inferior\n",
    "\n",
    "lower_bound_price_nuevo = q1_price.mean() * 0.25\n",
    "print(\"El nuevo límite inferior es\", lower_bound_price_nuevo)\n",
    "\n",
    "outlier_mask_up = data_sin_outliers_superficie.precio_usd_por_m2 < higher_bound_price\n",
    "outlier_mask_down = data_sin_outliers_superficie.precio_usd_por_m2 > lower_bound_price_nuevo\n",
    "outlier_mask = np.logical_and(outlier_mask_up, outlier_mask_down)\n",
    "data_sin_outliers_price = data_sin_outliers_superficie[outlier_mask]\n",
    "data_sin_outliers_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_ambientes = data_sin_outliers_price.ambientes_train.quantile(0.25)\n",
    "q2_ambientes = data_sin_outliers_price.ambientes_train.quantile(0.5)\n",
    "q3_ambientes = data_sin_outliers_price.ambientes_train.quantile(0.75)\n",
    "\n",
    "higher_bound_ambientes = q3_ambientes + 1.5 * (q3_ambientes - q1_ambientes)\n",
    "lower_bound_ambientes = q1_ambientes - 1.5 * (q3_ambientes - q1_ambientes)\n",
    "print(\"El límite inferior es \", lower_bound_ambientes, \" y el superior es \", higher_bound_ambientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considerando que el límite inferior da negativo, no se usará pues solo tiene lógica que un departamento tenga al menos 1 ambiente.\n",
    "\n",
    "mask = data_sin_outliers_price.ambientes_train > 7\n",
    "mask2 = data_sin_outliers_price.loc[mask, :]\n",
    "data_sin_outliers_ambientes = data_sin_outliers_price.drop(mask2.index, axis = 0)\n",
    "data_sin_outliers_ambientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Exportación del dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = data_sin_outliers_ambientes.copy()\n",
    "data_final.to_csv('data_final.csv', index = False, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
